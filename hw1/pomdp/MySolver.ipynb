{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we go ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "from itertools import count\n",
    "\n",
    "from env import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belief update\n",
    "\n",
    "![belief_update](imgs/belief_update.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief(b: np.ndarray, a: Actions, o: Obs, env: MyEnv) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the next belief state from the current belief state, applied action\n",
    "    and received observation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        Current belief state\n",
    "    a\n",
    "        Applied action\n",
    "    o\n",
    "        Observation received\n",
    "    env\n",
    "        Tiger Environment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Next belief state.\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract transition probability matrix\n",
    "    # adn observation probability matrix\n",
    "    T, O = env.T, env.O\n",
    "    \n",
    "    # get states, actions & observations\n",
    "    states, actions, obs = env.states, env.actions, env.obs\n",
    "    \n",
    "    # compute the next belief state\n",
    "    b_prime = np.zeros_like(b)\n",
    "    \n",
    "    for s_prime in states:\n",
    "        _sum = 0\n",
    "        for s in states:\n",
    "            _sum += T[a][s, s_prime] * b[s]\n",
    "        \n",
    "        b_prime[s_prime] = O[a][s_prime, o] * _sum\n",
    "        \n",
    "    # normalize\n",
    "    if b_prime.sum() > 0:\n",
    "        b_prime /= b_prime.sum()\n",
    "        return b_prime\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate belief states\n",
    "\n",
    "Most of the states will result in duplicates. It is important to remove the duplicates in the generation process. This can be implemented by computing the $L_p$ distance between two belief states. You are free to choose any norm you like (e.g. $L_1$, $L_2$, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate(b: np.ndarray, buff: List[np.ndarray], eps: float = 1e-8) -> bool:\n",
    "    \"\"\"\n",
    "    Checks whether the belief is already in the buffer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        belief to check if it already exists\n",
    "    buff\n",
    "        buffer of beliefs to check against\n",
    "    eps\n",
    "        distance threshold\n",
    "    \"\"\"\n",
    "    dist = np.array([np.linalg.norm(b - x) for x in buff])\n",
    "    return any(dist < eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beliefs generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_beliefs(b: np.ndarray, env:MyEnv) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates a list of possible beliefs that can result\n",
    "    form the current belief passed as argument\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        current belief\n",
    "    env\n",
    "        environment\n",
    "    \"\"\"\n",
    "    # get the list of possible actions\n",
    "    acts, obs = env.actions, env.obs\n",
    "    buff = []\n",
    "    \n",
    "    # go through all the actions\n",
    "    for a in acts:\n",
    "        \n",
    "        # go through all the observations\n",
    "        for o in obs:\n",
    "            # update current belief according to the\n",
    "            # current action and observation using \n",
    "            # the update_belief function previously implemented\n",
    "            b_prime = update_belief(b, a, o, env)\n",
    "            \n",
    "            if b_prime is None:\n",
    "                continue\n",
    "            \n",
    "            # add the new belief to the buffer only\n",
    "            # if it is not a duplicate\n",
    "            if not check_duplicate(b_prime, buff):\n",
    "                buff.append(b_prime)\n",
    "    \n",
    "    return buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_beliefs(b_init: np.ndarray, env: MyEnv) -> List[List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Generate all the possible belief that can result\n",
    "    in the maximum steps allowed\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b_init\n",
    "        initial belief (we're going to use the [0.5, 0.5] for this lab).\n",
    "    env\n",
    "        environment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List of lists of belief, meaning that for each step\n",
    "    we will have a list of belief.\n",
    "    E.g.\n",
    "    [\n",
    "        [b_init],            ---> initial belief (level 1)\n",
    "        [b00, b01, b02, ...] ---> those result from the initial belief (level 2)\n",
    "        [b10, b11, b12, ...] ---> those result form the beliefs from the second level (level 3)\n",
    "        ....\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract the maximum number of steps allowe\n",
    "    # by the environment\n",
    "    max_num_steps = env.max_num_steps\n",
    "    \n",
    "    # initialize storing buffer by adding the \n",
    "    # list containing the initial belief\n",
    "    buff = [[b_init]]\n",
    "    \n",
    "    # for  the maximum steps allowed\n",
    "    for step in range(1, max_num_steps):\n",
    "        # buffer for the next level of beliefs\n",
    "        next_buff = []\n",
    "        \n",
    "        # go through all beliefs from the previous level\n",
    "        # and generate new ones\n",
    "        for b in buff[step - 1]:\n",
    "            # generate all the belief that can result for\n",
    "            # belief b (apply get_next_belief previously implemented)\n",
    "            tmp_buff = get_next_beliefs(b, env)\n",
    "            \n",
    "            # we have to check if the new beliefs\n",
    "            # don't exist already in the next level buffer\n",
    "            # so we don't add duplicates\n",
    "            for b_prime in tmp_buff:\n",
    "                if not check_duplicate(b_prime, next_buff):\n",
    "                    next_buff.append(b_prime)\n",
    "            \n",
    "        # add the new level of beliefs\n",
    "        buff.append(next_buff)\n",
    "    \n",
    "    return buff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-based value backup (1)\n",
    "\n",
    "![pbvb1](imgs/pbvb1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gamma_a_star(a: Actions, env: MyEnv) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    a\n",
    "        current action\n",
    "    env\n",
    "        environment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    alpha^(a, *) vector. This in an array\n",
    "    of dimension: # of states and can be extracted\n",
    "    directly from the rewards matrix\n",
    "    \"\"\"\n",
    "    return env.R[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-based value backup (2)\n",
    "\n",
    "![pbvb2](imgs/pbvb2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gamma_a_o(a: Actions, o: Obs, V_prime: List[np.array], env: MyEnv, gamma: float=0.9):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    a\n",
    "        action\n",
    "    o\n",
    "        observation\n",
    "    V_prime\n",
    "        list of alpha vectors from the next step\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discounting factor\n",
    "    \"\"\"\n",
    "    # get transition, observation and reward matrix\n",
    "    T, O, R = env.T, env.O, env.R\n",
    "    \n",
    "    # get posible states, actions and observations\n",
    "    states, actions, obs = env.states, env.actions, env.obs\n",
    "    \n",
    "    # buffer of next gamma_ao vectors\n",
    "    gamma_a_o = []\n",
    "    \n",
    "    # go through all alpha_prime vectors from V_prime\n",
    "    # print(V_prime)\n",
    "    for alpha_prime in V_prime:\n",
    "        # define the new alpha_a_o vector\n",
    "        alpha_a_o = np.zeros((len(states)))\n",
    "        \n",
    "        # go throguh all states (s)\n",
    "        for s in states:\n",
    "            # go through all states (s_prime)\n",
    "            for s_prime in states:\n",
    "                alpha_a_o[s] += T[a][s, s_prime] * O[a][s_prime, o] * alpha_prime[s_prime]\n",
    "        \n",
    "        # append the new alpha_a_o vector to the buffer\n",
    "        gamma_a_o.append(gamma * alpha_a_o)\n",
    "    \n",
    "    return gamma_a_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-based value backup (3)\n",
    "\n",
    "![pbvb3](imgs/pbvb3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gamma_a_b(b: np.ndarray, \n",
    "                  V_prime: List[np.ndarray], \n",
    "                  env: MyEnv, \n",
    "                  gamma: float=0.9) -> Dict[Actions, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        belief state\n",
    "    V_prime\n",
    "        list of alpha vector from the next step\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discounting factor\n",
    "    \"\"\"\n",
    "    # get all posible actions and observations\n",
    "    A, O = env.actions, env.obs\n",
    "    \n",
    "    # define gamma_a_b buffer \n",
    "    gamma_a_b = {}\n",
    "    \n",
    "    # go through all actions\n",
    "    for a in A:\n",
    "        # get the gamma_a_star vectors form the previously implemented function\n",
    "        gamma_a_star = get_gamma_a_star(a, env)\n",
    "        \n",
    "        # define accumulator accumulator\n",
    "        sum_gamma_a_o = np.zeros_like(gamma_a_star, dtype=np.float)\n",
    "        \n",
    "        # go through all the observations\n",
    "        for o in O:\n",
    "            # get gamma_a_o from the previously implementd function\n",
    "            gamma_a_o = get_gamma_a_o(a, o, V_prime, env, gamma)\n",
    "            \n",
    "            # need to do a maximization\n",
    "            best_alpha = None\n",
    "            best_score = -np.inf\n",
    "            \n",
    "            # go through all alphas from gamma_a_o\n",
    "            for alpha in gamma_a_o:\n",
    "                # compute the score by dot product between\n",
    "                # alpha and current belief b\n",
    "                score = np.dot(alpha, b)\n",
    "                \n",
    "                # update the best score and alpha\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_alpha = alpha\n",
    "            \n",
    "            # add best alpha to the summation\n",
    "            # notice that if V_prime is empty (for the last step)\n",
    "            # we don't have any best_alpha\n",
    "            if best_alpha is not None:\n",
    "                sum_gamma_a_o += best_alpha\n",
    "        \n",
    "        # add the reward vector to the accumulator\n",
    "        sum_gamma_a_o += gamma_a_star\n",
    "        \n",
    "        # store mapping between action and accumulator\n",
    "        gamma_a_b[a] = sum_gamma_a_o\n",
    "\n",
    "    return gamma_a_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-based value backup (4)\n",
    "\n",
    "![pbvb4](imgs/pbvb4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_V(B, V_prime, env: MyEnv, gamma: float=0.9) -> Tuple[List[np.ndarray], Dict, List[float]]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    B\n",
    "        List of beliefs (per leve)\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discount factor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple containing the  the new V list, best policy for the current level, \n",
    "    and the best scores\n",
    "    \"\"\"\n",
    "    # define policy dictionary\n",
    "    policy = {}\n",
    "    \n",
    "    # define V and score buffers\n",
    "    V, scores = [], []\n",
    "    \n",
    "    # go through all beliefs\n",
    "    for b in B:\n",
    "        # get gamma_a_b dictionary form the previous implemented function\n",
    "        gamma_a_b = get_gamma_a_b(b, V_prime, env, gamma)\n",
    "        \n",
    "        # variables for maximization\n",
    "        best_a = None\n",
    "        best_score = -np.inf\n",
    "        best_gamma_a_b = None\n",
    "        \n",
    "        # go through all actions from gamma_a_b\n",
    "        for a in gamma_a_b:\n",
    "            # compute score by dot product between\n",
    "            # the gamma_a_b corresponding to a and the current belief\n",
    "            score = np.dot(b, gamma_a_b[a])\n",
    "            \n",
    "            # update score if better and\n",
    "            # remeber the action and the alpha\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_a = a\n",
    "                best_gamma_a_b = gamma_a_b[a]\n",
    "        \n",
    "        # remeber the action to be applied\n",
    "        # for the current belief state \n",
    "        policy[tuple(b)] = best_a\n",
    "        \n",
    "        # add best gamma_a_b to the V\n",
    "        V.append(best_gamma_a_b)\n",
    "        \n",
    "        # also remebere the best score\n",
    "        scores.append(best_score)   \n",
    "    \n",
    "    return V, policy, scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-based value backup (all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_based_value_backup(env: MyEnv, \n",
    "                             gamma: float=0.9):\n",
    "    \"\"\"\n",
    "    Point-based value backup algorithm for POMDP\n",
    "    Link: http://www.cs.cmu.edu/~ggordon/jpineau-ggordon-thrun.ijcai03.pdf\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discount factor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Best policies per level. Each element\n",
    "    corresponds to a level\n",
    "    \"\"\"\n",
    "    \n",
    "    # define initial belief states\n",
    "    b_init = np.array([0.5, 0, 0, 0, 0.5])\n",
    "    \n",
    "    # generate the list of all possible beliefs per lever\n",
    "    B = generate_all_beliefs(b_init, env)\n",
    "    \n",
    "    # need to reverse the list cause we are starting\n",
    "    # from the last possible acton\n",
    "    B = reversed(B)\n",
    "    \n",
    "    # initail list of best gamma_a_b\n",
    "    V = []\n",
    "    \n",
    "    # buffer of policies and V vectors\n",
    "    policies = {}\n",
    "    \n",
    "    # for each level and each set of beliefs\n",
    "    for i, bs in enumerate(B):\n",
    "        # get the V's, policy and the best scores\n",
    "        V, policy, scores = get_V(bs, V, env)\n",
    "        \n",
    "        # store results\n",
    "        policies[env.max_num_steps - i - 1] = {\n",
    "            \"policy\": policy,\n",
    "            \"V\": V,\n",
    "            \"scores\": scores\n",
    "        }\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = MyEnv(max_num_steps=5)\n",
    "\n",
    "# solve environment\n",
    "policies = point_based_value_backup(env=env, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the policy makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Step 0 ======== \n",
      "{(0.5, 0.0, 0.0, 0.0, 0.5): <Actions.LEFT: 0>}\n",
      "\n",
      "\n",
      "========== Step 1 ======== \n",
      "{(0.0, 0.0, 0.0, 0.0, 1.0): <Actions.LEFT: 0>,\n",
      " (0.0, 0.0, 0.0, 1.0, 0.0): <Actions.LEFT: 0>,\n",
      " (0.0, 1.0, 0.0, 0.0, 0.0): <Actions.RIGHT: 1>,\n",
      " (0.5, 0.0, 0.0, 0.0, 0.5): <Actions.LEFT: 0>,\n",
      " (1.0, 0.0, 0.0, 0.0, 0.0): <Actions.RIGHT: 1>}\n",
      "\n",
      "\n",
      "========== Step 2 ======== \n",
      "{(0.0, 0.0, 0.0, 0.0, 1.0): <Actions.LEFT: 0>,\n",
      " (0.0, 0.0, 0.0, 1.0, 0.0): <Actions.LEFT: 0>,\n",
      " (0.0, 0.0, 1.0, 0.0, 0.0): <Actions.NOOP: 2>,\n",
      " (0.0, 1.0, 0.0, 0.0, 0.0): <Actions.RIGHT: 1>,\n",
      " (0.5, 0.0, 0.0, 0.0, 0.5): <Actions.LEFT: 0>,\n",
      " (1.0, 0.0, 0.0, 0.0, 0.0): <Actions.RIGHT: 1>}\n",
      "\n",
      "\n",
      "========== Step 3 ======== \n",
      "{(0.0, 0.0, 0.0, 0.0, 1.0): <Actions.LEFT: 0>,\n",
      " (0.0, 0.0, 0.0, 1.0, 0.0): <Actions.LEFT: 0>,\n",
      " (0.0, 0.0, 1.0, 0.0, 0.0): <Actions.NOOP: 2>,\n",
      " (0.0, 1.0, 0.0, 0.0, 0.0): <Actions.RIGHT: 1>,\n",
      " (0.5, 0.0, 0.0, 0.0, 0.5): <Actions.LEFT: 0>,\n",
      " (1.0, 0.0, 0.0, 0.0, 0.0): <Actions.RIGHT: 1>}\n",
      "\n",
      "\n",
      "========== Step 4 ======== \n",
      "{(0.0, 0.0, 0.0, 0.0, 1.0): <Actions.LEFT: 0>,\n",
      " (0.0, 0.0, 0.0, 1.0, 0.0): <Actions.LEFT: 0>,\n",
      " (0.0, 0.0, 1.0, 0.0, 0.0): <Actions.NOOP: 2>,\n",
      " (0.0, 1.0, 0.0, 0.0, 0.0): <Actions.RIGHT: 1>,\n",
      " (0.5, 0.0, 0.0, 0.0, 0.5): <Actions.LEFT: 0>,\n",
      " (1.0, 0.0, 0.0, 0.0, 0.0): <Actions.LEFT: 0>}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in range(env.max_num_steps):\n",
    "    print(\"========== Step %d ======== \" % (step, ))\n",
    "    pprint(policies[step]['policy'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the agent in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'policy': {(0.5, 0.0, 0.0, 0.0, 0.5): <Actions.LEFT: 0>}, 'V': [array([0.81, 0.81, 0.  , 1.  , 1.71])], 'scores': [1.26]}\n",
      "Before [0.5 0.  0.  0.  0.5] action Actions.LEFT\n",
      "After [1. 0. 0. 0. 0.]\n",
      "Before [1. 0. 0. 0. 0.] action Actions.RIGHT\n",
      "After [0. 1. 0. 0. 0.]\n",
      "Episode 0, Score: 1.00\n",
      "\t* Actions: ['Left', 'Right', 'Right']\n",
      "\t* Obs: ['Three walls', 'Two walls', 'Two walls']\n",
      "\n",
      "\n",
      "Before [0.5 0.  0.  0.  0.5] action Actions.LEFT\n",
      "After [0. 0. 0. 1. 0.]\n",
      "Before [0. 0. 0. 1. 0.] action Actions.LEFT\n",
      "After [0. 0. 1. 0. 0.]\n",
      "Episode 1, Score: 2.00\n",
      "\t* Actions: ['Left', 'Left', 'NoOp']\n",
      "\t* Obs: ['Two walls', 'Two walls', 'Two walls']\n",
      "\n",
      "\n",
      "Before [0.5 0.  0.  0.  0.5] action Actions.LEFT\n",
      "After [0. 0. 0. 1. 0.]\n",
      "Before [0. 0. 0. 1. 0.] action Actions.LEFT\n",
      "After [0. 0. 1. 0. 0.]\n",
      "Episode 2, Score: 2.00\n",
      "\t* Actions: ['Left', 'Left', 'NoOp']\n",
      "\t* Obs: ['Two walls', 'Two walls', 'Two walls']\n",
      "\n",
      "\n",
      "Before [0.5 0.  0.  0.  0.5] action Actions.LEFT\n",
      "After [1. 0. 0. 0. 0.]\n",
      "Before [1. 0. 0. 0. 0.] action Actions.RIGHT\n",
      "After [0. 1. 0. 0. 0.]\n",
      "Episode 3, Score: 1.00\n",
      "\t* Actions: ['Left', 'Right', 'Right']\n",
      "\t* Obs: ['Three walls', 'Two walls', 'Two walls']\n",
      "\n",
      "\n",
      "Before [0.5 0.  0.  0.  0.5] action Actions.LEFT\n",
      "After [0. 0. 0. 1. 0.]\n",
      "Before [0. 0. 0. 1. 0.] action Actions.LEFT\n",
      "After [0. 0. 1. 0. 0.]\n",
      "Episode 4, Score: 2.00\n",
      "\t* Actions: ['Left', 'Left', 'NoOp']\n",
      "\t* Obs: ['Two walls', 'Two walls', 'Two walls']\n",
      "\n",
      "\n",
      "Before [0.5 0.  0.  0.  0.5] action Actions.LEFT\n",
      "After [0. 0. 0. 1. 0.]\n",
      "Before [0. 0. 0. 1. 0.] action Actions.LEFT\n",
      "After [0. 0. 1. 0. 0.]\n",
      "Episode 5, Score: 2.00\n",
      "\t* Actions: ['Left', 'Left', 'NoOp']\n",
      "\t* Obs: ['Two walls', 'Two walls', 'Two walls']\n",
      "\n",
      "\n",
      "Before [0.5 0.  0.  0.  0.5] action Actions.LEFT\n",
      "After [1. 0. 0. 0. 0.]\n",
      "Before [1. 0. 0. 0. 0.] action Actions.RIGHT\n",
      "After [0. 1. 0. 0. 0.]\n",
      "Episode 6, Score: 1.00\n",
      "\t* Actions: ['Left', 'Right', 'Right']\n",
      "\t* Obs: ['Three walls', 'Two walls', 'Two walls']\n",
      "\n",
      "\n",
      "Before [0.5 0.  0.  0.  0.5] action Actions.LEFT\n",
      "After [0. 0. 0. 1. 0.]\n",
      "Before [0. 0. 0. 1. 0.] action Actions.LEFT\n",
      "After [0. 0. 1. 0. 0.]\n",
      "Episode 7, Score: 2.00\n",
      "\t* Actions: ['Left', 'Left', 'NoOp']\n",
      "\t* Obs: ['Two walls', 'Two walls', 'Two walls']\n",
      "\n",
      "\n",
      "Before [0.5 0.  0.  0.  0.5] action Actions.LEFT\n",
      "After [0. 0. 0. 1. 0.]\n",
      "Before [0. 0. 0. 1. 0.] action Actions.LEFT\n",
      "After [0. 0. 1. 0. 0.]\n",
      "Episode 8, Score: 2.00\n",
      "\t* Actions: ['Left', 'Left', 'NoOp']\n",
      "\t* Obs: ['Two walls', 'Two walls', 'Two walls']\n",
      "\n",
      "\n",
      "Before [0.5 0.  0.  0.  0.5] action Actions.LEFT\n",
      "After [1. 0. 0. 0. 0.]\n",
      "Before [1. 0. 0. 0. 0.] action Actions.RIGHT\n",
      "After [0. 1. 0. 0. 0.]\n",
      "Episode 9, Score: 1.00\n",
      "\t* Actions: ['Left', 'Right', 'Right']\n",
      "\t* Obs: ['Three walls', 'Two walls', 'Two walls']\n",
      "\n",
      "\n",
      "=================\n",
      "Avg score: 1.60\n"
     ]
    }
   ],
   "source": [
    "# define environment\n",
    "# you can change the number of steps\n",
    "env = MyEnv(max_num_steps=3)\n",
    "\n",
    "# solve environment\n",
    "policies = point_based_value_backup(env=env, gamma=0.9)\n",
    "print(policies[0])\n",
    "\n",
    "# do a few experiments\n",
    "scores = []\n",
    "num_experiments = 10\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    # reset environment\n",
    "    env.reset()\n",
    "    \n",
    "    # initial belief state\n",
    "    b = np.array([0.5, 0, 0, 0, 0.5])\n",
    "    \n",
    "    # score variable & buffer actions\n",
    "    score = 0\n",
    "    acts, obs = [], []\n",
    "    \n",
    "    for step in count():\n",
    "        # interact with the environment\n",
    "        b = get_closest_belief(policies[step][\"policy\"], b)\n",
    "        a = policies[step][\"policy\"][tuple(b)]\n",
    "        o, r, done, _ = env.step(a)\n",
    "        \n",
    "        # update score, acts & obs\n",
    "        score += r\n",
    "        acts.append(a)\n",
    "        obs.append(o)\n",
    "        \n",
    "        # break if environment completed\n",
    "        if done:\n",
    "            acts = [env.action_mapping[a] for a in acts]\n",
    "            obs = [env.obs_mapping[o] for o in obs]\n",
    "            \n",
    "            print(\"Episode %d, Score: %.2f\" % (i, score))\n",
    "            print(\"\\t* Actions:\", acts)\n",
    "            print(\"\\t* Obs:\", obs)\n",
    "            print(\"\\n\")\n",
    "            break\n",
    "        \n",
    "        # update belief\n",
    "        print(\"Before\", b, \"action\", a)\n",
    "        b = update_belief(b=b, a=a, o=o, env=env)\n",
    "        print(\"After\", b)\n",
    "    \n",
    "    # save score\n",
    "    scores.append(score)\n",
    "    \n",
    "# report mean score\n",
    "print(\"=================\")\n",
    "print(\"Avg score: %.2f\" % (np.mean(scores), ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
