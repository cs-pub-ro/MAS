{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from itertools import count\n",
    "from typing import Union, Tuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size: int = 10000):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        size\n",
    "            Maximum number of transitions store in the buffer.\n",
    "            If the buffer overflows, older states are dropped.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.size    = size\n",
    "        self.length  = 0\n",
    "        self.idx     = -1\n",
    "        \n",
    "        # define buffers\n",
    "        self.states        = None\n",
    "        self.states_next   = None\n",
    "        self.actions       = None\n",
    "        self.rewards       = None\n",
    "        self.done          = None\n",
    "        \n",
    "    def store(self, \n",
    "              s: Union[torch.Tensor, np.ndarray], \n",
    "              a: int, \n",
    "              r: float, \n",
    "              s_next: Union[torch.Tensor, np.ndarray],\n",
    "              done: bool):\n",
    "        \n",
    "        \"\"\"\n",
    "        Stores one sample of experience\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        s\n",
    "            Tensor encoding the current state.\n",
    "        a\n",
    "            Current action.\n",
    "        r\n",
    "            Current reward.\n",
    "        s_next\n",
    "            Tensor encoding the next state.\n",
    "        done\n",
    "            Done signal.\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize buffers\n",
    "        if self.states is None:\n",
    "            self.states      = torch.zeros([self.size] + list(s.shape))\n",
    "            self.states_next = torch.zeros_like(self.states)\n",
    "            self.actions     = torch.zeros((self.size, ))\n",
    "            self.rewards     = torch.zeros((self.size, ))\n",
    "            self.done        = torch.zeros((self.size, ))\n",
    "        \n",
    "        self.idx = (self.idx + 1) % self.size\n",
    "        self.length = min(self.length + 1, self.size)\n",
    "        \n",
    "        if type(s) != Tensor:\n",
    "            s = Tensor(s)\n",
    "            \n",
    "        if type(s_next) != Tensor:\n",
    "            s_next = Tensor(s_next)\n",
    "        \n",
    "        # store the experience\n",
    "        self.states[self.idx]       = s\n",
    "        self.states_next[self.idx]  = s_next\n",
    "        self.actions[self.idx]      = a\n",
    "        self.rewards[self.idx]      = r\n",
    "        self.done[self.idx ]        = int(done)\n",
    "        \n",
    "    def sample(self, batch_size: int = 128) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Sample a batch of experience\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size\n",
    "            Number of experience to sample\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of tensor consisting of a batch of states, actions, rewards, next states, done\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.length >= batch_size, \"Can not sample from the buffer yet\"\n",
    "        indices = np.random.choice(a=np.arange(self.length), size=batch_size, replace=False)\n",
    "        \n",
    "        s      = self.states[indices]\n",
    "        s_next = self.states_next[indices]\n",
    "        a      = self.actions[indices]\n",
    "        r      = self.rewards[indices]\n",
    "        done   = self.done[indices]\n",
    "        return s, a, r, s_next, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network achitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_RAM(nn.Module):\n",
    "    def __init__(self, in_features: int, num_actions: int):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network for testing algorithm\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features\n",
    "            number of features of input.\n",
    "        num_actions\n",
    "            number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN_RAM, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # define architecture\n",
    "        self.fc1 = nn.Linear(in_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_generator(max_eps: float=1.0, min_eps: float=0.1, max_iter: int = 10000):\n",
    "    crt_iter = -1\n",
    "    \n",
    "    while True:\n",
    "        crt_iter += 1\n",
    "        frac = min(crt_iter/max_iter, 1)\n",
    "        eps = (1 - frac) * max_eps + frac * min_eps\n",
    "        yield eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epilson_greedy_action(Q: nn.Module, s: Tensor, eps: float):\n",
    "    rand = np.random.rand()\n",
    "    \n",
    "    # with prob eps select a random action\n",
    "    if rand < eps:\n",
    "        return np.random.choice(np.arange(Q.num_actions))\n",
    "    \n",
    "    # select best action\n",
    "    with torch.no_grad():\n",
    "        output = Q(s).argmax(dim=1).item()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_target(\n",
    "    Q: nn.Module,\n",
    "    target_Q: nn.Module,\n",
    "    r_batch: Tensor,\n",
    "    s_next_batch: Tensor,\n",
    "    done_batch: Tensor,\n",
    "    gamma: float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes DQN target\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Q\n",
    "        Behavior Q network.\n",
    "    target_Q\n",
    "        Target Q network.\n",
    "    r_batch\n",
    "        Batch of rewards.\n",
    "    s_next_bacth\n",
    "        Batch of next states.\n",
    "    done_batch\n",
    "        Batcho of done flag (1 means the episoded finished).\n",
    "    gamma\n",
    "        Discount factor.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Batch of DQN targets\n",
    "    \"\"\"\n",
    "    # cmpute next Q value based on which action gives max Q values\n",
    "    # detach variable from the current graph since we don't want gradients for next Q to propagated\n",
    "    next_max_Q    = torch.max(target_Q(s_next_batch).detach(), dim=1)[0]\n",
    "    next_Q_values = (1 - done_batch) * next_max_Q\n",
    "    return r_batch + (gamma * next_Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddqn_target(\n",
    "    Q: nn.Module,\n",
    "    target_Q: nn.Module,\n",
    "    r_batch: Tensor,\n",
    "    s_next_batch: Tensor,\n",
    "    done_batch: Tensor,\n",
    "    gamma: float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes DQN target\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Q\n",
    "        Behavior Q network.\n",
    "    target_Q\n",
    "        Target Q network.\n",
    "    r_batch\n",
    "        Batch of rewards.\n",
    "    s_next_bacth\n",
    "        Batch of next states.\n",
    "    done_batch\n",
    "        Batcho of done flag (1 means the episoded finished).\n",
    "    gamma\n",
    "        Discount factor.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Batch of DQN targets\n",
    "    \"\"\"\n",
    "    # cmpute next Q value based on which action gives max Q values\n",
    "    # detach variable from the current graph since we don't want gradients for next Q to propagated\n",
    "    next_max_Q = target_Q(s_next_batch).gather(1, Q(s_next_batch).argmax(dim=1, keepdim=True))\n",
    "    next_max_Q = next_max_Q.view(-1).detach()\n",
    "    next_Q_values = (1 - done_batch) * next_max_Q\n",
    "\n",
    "    # Compute the target of the current Q values\n",
    "    return r_batch + (gamma * next_Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Learning Alogrithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learing(\n",
    "    env: gym.Env,\n",
    "    targert_function: Callable,\n",
    "    batch_size: int = 128,\n",
    "    gamma: float = 0.99,\n",
    "    replay_buffer_size=10000,\n",
    "    num_episodes: int = 100000,\n",
    "    learning_starts: int = 1000,\n",
    "    learning_freq: int = 4,\n",
    "    target_update_freq: int = 100,\n",
    "    log_every: int = 100):\n",
    "\n",
    "    \"\"\"\n",
    "    DQN Learning\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env\n",
    "        gym environment to train on.\n",
    "    target_function\n",
    "        Function that computes the Q network target. For DQN - dqn_target, for DDQN - ddqn_target.\n",
    "    batch_size:\n",
    "        How many transitions to sample each time experience is replayed.\n",
    "    gamma\n",
    "        Discount Factor\n",
    "    replay_buffer_size\n",
    "        Replay buffer size.\n",
    "    num_episodes\n",
    "        number of episodes to run\n",
    "    learning_starts: int\n",
    "        After how many environment steps to start replaying experiences\n",
    "    learning_freq: int\n",
    "        How many steps of environment to take between every experience replay\n",
    "    target_update_freq: int\n",
    "        How many experience replay rounds (not steps!) to perform between\n",
    "        each update to the target Q network\n",
    "    log_every:\n",
    "        Logging interval\n",
    "    \"\"\"\n",
    "    # This means we are running on low-dimensional observations (e.g. RAM)\n",
    "    input_arg = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # define device \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize target q function and q function\n",
    "    Q = DQN_RAM(input_arg, num_actions).to(device)\n",
    "    target_Q = DQN_RAM(input_arg, num_actions).to(device)\n",
    "      \n",
    "    # Construct Q network optimizer function\n",
    "    optimizer = optim.Adam(Q.parameters(), lr=1e-3)\n",
    "    \n",
    "    # define criterion\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Construct the replay buffer\n",
    "    replay_buffer = ReplayBuffer()\n",
    "    \n",
    "    # define epsilon scheduler\n",
    "    eps_scheduler = iter(eps_generator())\n",
    "    \n",
    "    # define statistics buffer, total number of steps and total number of updates performed\n",
    "    all_episode_rewards = []\n",
    "    total_steps = 0\n",
    "    num_param_updates = 0\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # reset environment\n",
    "        s = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for _ in count():\n",
    "            # increse total number of steps\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Choose random action if not yet start learning\n",
    "            if total_steps > learning_starts:\n",
    "                eps = next(eps_scheduler)\n",
    "                s = torch.tensor(s).view(1, -1).float().to(device)\n",
    "                a = select_epilson_greedy_action(Q, s, eps)\n",
    "            else:\n",
    "                a = np.random.choice(np.arange(num_actions))\n",
    "\n",
    "            # advance one step\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            \n",
    "            # update episode rewards\n",
    "            episode_reward += r\n",
    "\n",
    "            # store other info in replay memory\n",
    "            replay_buffer.store(s, a, r, s_next, done)\n",
    "\n",
    "            # Resets the environment when reaching an episode boundary.\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # update state\n",
    "            s = s_next\n",
    "\n",
    "            # perform experience replay and train the network.\n",
    "            if (total_steps > learning_starts and total_steps % learning_freq == 0):\n",
    "                for _ in range(learning_freq):\n",
    "                    # sample experinence from the replay buffer\n",
    "                    s_batch, a_batch, r_batch, s_next_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "                    # send everything to device\n",
    "                    s_batch      = s_batch.float().to(device)\n",
    "                    a_batch      = a_batch.long().to(device)\n",
    "                    r_batch      = r_batch.float().to(device)\n",
    "                    s_next_batch = s_next_batch.float().to(device)\n",
    "                    done_batch   = done_batch.long().to(device)\n",
    "\n",
    "                    # comput the q values according to the states and actions\n",
    "                    Q_values = Q(s_batch).gather(1, a_batch.unsqueeze(1)).view(-1)\n",
    "\n",
    "                    # Compute the target of the current Q values\n",
    "                    target_Q_values = targert_function(Q, target_Q, r_batch, s_next_batch, done_batch, gamma)\n",
    "\n",
    "                    # compute loss\n",
    "                    loss = criterion(target_Q_values, Q_values)\n",
    "\n",
    "                    # Clear previous gradients before backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # increase number of updates\n",
    "                    num_param_updates += 1\n",
    "\n",
    "                    # Periodically update the target network by Q network to target Q network\n",
    "                    if num_param_updates % target_update_freq == 0:\n",
    "                        target_Q.load_state_dict(Q.state_dict())\n",
    "\n",
    "        # append total reward culumated\n",
    "        all_episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # log average reward over the last 100 episodes\n",
    "        if episode % log_every == 0 and total_steps > learning_starts:\n",
    "            mean_episode_reward = np.mean(all_episode_rewards[-100:])\n",
    "            print(\"Episode: %d, Mean reward: %.2f, Eps: %.2f\" % (episode, mean_episode_reward, eps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gym env\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# DQN learning\n",
    "learing(\n",
    "    env=env,                          # gym environmnet\n",
    "    targert_function=dqn_target,      # dqn target construction\n",
    "    batch_size=128,                   # q-network update batch size\n",
    "    gamma=0.99,                       # discount factor\n",
    "    replay_buffer_size=10000,         # size of the replay buffer\n",
    "    num_episodes=1000,                # number of episodes to run\n",
    "    learning_starts=1000,             # number of initial random actions (exploration)\n",
    "    learning_freq=4,                  # frequency of the update\n",
    "    target_update_freq=100,           # number of gradient steps after which the target network is updated\n",
    "    log_every=100                     # logging interval. returns the mean reward per episode.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gym env\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# DQN learning\n",
    "learing(\n",
    "    env=env,                          # gym environmnet\n",
    "    targert_function=ddqn_target,     # dqn target construction\n",
    "    batch_size=128,                   # q-network update batch size\n",
    "    gamma=0.99,                       # discount factor\n",
    "    replay_buffer_size=10000,         # size of the replay buffer\n",
    "    num_episodes=1000,                # number of episodes to run\n",
    "    learning_starts=1000,             # number of initial random actions  (exploration)\n",
    "    learning_freq=4,                  # frequency of the update\n",
    "    target_update_freq=100,           # number of gradient steps after which the target network is updated\n",
    "    log_every=100                     # logging interval. returns the mean reward per episode.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
